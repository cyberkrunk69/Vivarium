[
  {
    "task": "IMPORTANCE SCORING: Add LLM-based importance to learned_lessons.json\n\nFrom research_brief_self_improvement.md (Generative Agents technique):\n\n1. Read memory_synthesis.py - it has compute_importance() but it's heuristic-based\n2. Add LLM scoring prompt:\n   ```\n   On scale 1-10, where 1 is mundane (checking status) and 10 is significant\n   (fixing critical bug), rate: {lesson_content}\n   ```\n3. Update each lesson in learned_lessons.json to include 'importance' field\n4. Modify synthesize() to weight by importance when selecting reflections\n\nThe research shows this enables 70% better retrieval of relevant lessons.\n\nAppend lessons about importance scoring to learned_lessons.json.",
    "budget": 2.00,
    "model": "haiku"
  },
  {
    "task": "SHARED MESSAGE POOL: Implement MetaGPT-style worker communication\n\nFrom research_brief_multi_agent.md (MetaGPT technique):\n\n1. Create message_pool.py with:\n   - MessagePool class with JSON file storage\n   - publish(from_role, message_type, content, subscribers) method\n   - subscribe(role) -> get relevant messages\n   - Message types: TASK_ASSIGNMENT, EXECUTION_PLAN, CODE_ARTIFACT, TEST_RESULT, REVIEW_FEEDBACK\n\n2. Structure:\n   ```json\n   {\"messages\": [{\"id\": \"msg_001\", \"timestamp\": \"...\", \"from_role\": \"Planner\", \"type\": \"EXECUTION_PLAN\", \"content\": {...}, \"subscribers\": [\"Coder\"]}]}\n   ```\n\n3. Update roles.py to define ROLE_SUBSCRIPTIONS dict\n\nMetaGPT achieves 100% task completion with structured communication vs 60-70% without.\n\nAppend communication protocol lessons to learned_lessons.json.",
    "budget": 2.00,
    "model": "haiku"
  },
  {
    "task": "STRUCTURED ARTIFACTS: Add typed outputs for each role\n\nFrom research_brief_multi_agent.md (prevents hallucination cascading):\n\n1. Create artifacts/ directory\n2. Create artifacts/schemas.py with Pydantic models:\n   - TaskAssignment(task_id, description, assigned_to, acceptance_criteria)\n   - ExecutionPlan(task_id, steps: List[Step], estimated_complexity)\n   - CodeArtifact(task_id, files_modified, summary, tests_status)\n   - ReviewFeedback(task_id, approved: bool, issues: List[str], next_action)\n\n3. Update roles.py to specify output_schema for each role\n4. Validate role outputs before publishing to message_pool\n\nKey insight: 'Structured outputs prevent cascading hallucinations' - MetaGPT\n\nAppend artifact schema lessons to learned_lessons.json.",
    "budget": 2.00,
    "model": "haiku"
  },
  {
    "task": "DSPY SIGNATURES: Replace static prompts with learnable modules\n\nFrom research_brief_multi_agent.md (25-65% improvement):\n\n1. Read prompt_optimizer.py - it collects demos but doesn't use DSPy\n2. Create dspy_modules.py with:\n   ```python\n   class GrindSignature(dspy.Signature):\n       task: str = dspy.InputField()\n       context: str = dspy.InputField()\n       solution: str = dspy.OutputField()\n       summary: str = dspy.OutputField()\n   \n   class GrindModule(dspy.Module):\n       def __init__(self):\n           self.planner = dspy.ChainOfThought('task -> steps')\n           self.executor = dspy.Predict(GrindSignature)\n   ```\n\n3. Add teleprompter compilation:\n   - BootstrapFewShot from successful demonstrations\n   - Compile GrindModule with top demonstrations\n\nNote: If dspy not installed, create mock interface that can be swapped later.\n\nAppend DSPy integration lessons to learned_lessons.json.",
    "budget": 2.00,
    "model": "haiku"
  },
  {
    "task": "REFLECTION TRIGGERS: Implement automatic reflection synthesis\n\nFrom research_brief_self_improvement.md (Generative Agents technique):\n\n1. Read memory_synthesis.py - it has synthesize() but no automatic trigger\n2. Add reflection trigger to grind_spawner.py:\n   ```python\n   def maybe_reflect(session_count):\n       lessons = load_lessons()\n       recent = [l for l in lessons if recent_than_4_hours(l)]\n       importance_sum = sum(l.get('importance', 5) for l in recent)\n       if importance_sum > 150:  # Generative Agents threshold\n           synth = MemorySynthesis()\n           new_reflections = synth.synthesize()\n           print(f\"Generated {len(new_reflections)} reflections\")\n   ```\n\n3. Call maybe_reflect() after each grind session completes\n4. Log when reflections are generated\n\nThis consolidates lessons into higher-level insights automatically.\n\nAppend reflection trigger lessons to learned_lessons.json.",
    "budget": 2.00,
    "model": "haiku"
  },
  {
    "task": "SKILL EMBEDDING RETRIEVAL: Add semantic matching to skill library\n\nFrom research_brief_self_improvement.md (Voyager technique):\n\n1. Read skills/skill_registry.py - it has keyword matching but no embeddings\n2. Add embedding-based retrieval:\n   - For each skill, generate description embedding\n   - On retrieve_skill(task), embed task description\n   - Return top-k by cosine similarity\n\n3. If no embedding API available, implement TF-IDF fallback:\n   ```python\n   from sklearn.feature_extraction.text import TfidfVectorizer\n   from sklearn.metrics.pairwise import cosine_similarity\n   \n   def embed_skills(skills):\n       vectorizer = TfidfVectorizer()\n       descriptions = [s['description'] for s in skills]\n       return vectorizer.fit_transform(descriptions)\n   ```\n\n4. Fall back to keyword matching if TF-IDF unavailable\n\nVoyager achieves 3.3x more discoveries with embedding retrieval.\n\nAppend skill embedding lessons to learned_lessons.json.",
    "budget": 2.00,
    "model": "haiku"
  }
]
