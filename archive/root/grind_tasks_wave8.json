[
  {
    "task": "EMBEDDING: Create embedding-based skill retrieval\n\nUpgrade skill_registry.py to use semantic similarity:\n\n1. Read skill_registry.py and understand current keyword-based retrieval\n2. Add compute_embedding() function using simple TF-IDF or word vectors:\n   - For each skill, compute a normalized word frequency vector\n   - Store embeddings alongside skills in skill_library.json\n\n3. Add semantic_search(query, skills, top_k=3) function:\n   - Compute query embedding\n   - Find cosine similarity with all skill embeddings\n   - Return top-k most similar skills\n\n4. Update retrieve_skill() to use semantic search when keywords fail\n5. Add fallback: if no embedding match > 0.3, return None\n\nThis implements Voyager's embedding-based skill retrieval.\n\nAppend embedding retrieval lessons to learned_lessons.json.",
    "budget": 2.00,
    "model": "haiku"
  },
  {
    "task": "EMBEDDING: Create embedding-based lesson retrieval\n\nUpgrade memory_synthesis.py to use semantic similarity:\n\n1. Read memory_synthesis.py and understand current importance scoring\n2. Add compute_lesson_embedding(lesson_text) function:\n   - Normalize text, extract key terms\n   - Create simple TF-IDF style vector\n\n3. Add retrieve_relevant_lessons(query, lessons, top_k=5) function:\n   - Compute query embedding\n   - Score each lesson by: importance_weight * 0.4 + embedding_similarity * 0.6\n   - Return top-k lessons\n\n4. Call this from synthesize() to select lessons for reflection\n\nThis implements HippoRAG's semantic retrieval pattern.\n\nAppend embedding lesson lessons to learned_lessons.json.",
    "budget": 2.00,
    "model": "haiku"
  },
  {
    "task": "CRITIC: Create code review system\n\nCreate critic.py based on LATS/TextGrad patterns:\n\n1. Create critic.py with:\n   - CriticAgent class with review(code, context) method\n   - score_quality(code) -> 0.0-1.0 quality score\n   - generate_feedback(code, issues) -> improvement suggestions\n\n2. Quality scoring checks:\n   - Has proper error handling?\n   - Follows existing patterns in codebase?\n   - No obvious bugs or issues?\n   - Proper imports and dependencies?\n\n3. Add to grind_spawner.py (optional integration point):\n   - After task completion, run critic review\n   - Log quality score in grind log\n\nThis implements LATS/TextGrad critic feedback loop.\n\nAppend critic system lessons to learned_lessons.json.",
    "budget": 2.00,
    "model": "haiku"
  },
  {
    "task": "TREE SEARCH: Create solution exploration system\n\nCreate tree_search.py based on LATS (arXiv:2310.04406):\n\n1. Create tree_search.py with:\n   - TreeNode class with: state, action, children, value, visits\n   - expand_node(node) -> generate child states\n   - evaluate_node(node) -> score 0.0-1.0\n   - select_best_path(root) -> best sequence of actions\n\n2. Simple UCB selection:\n   - ucb_score = value/visits + sqrt(2*log(parent_visits)/visits)\n   - Select highest UCB child to explore\n\n3. Add run_tree_search(initial_state, max_expansions=10):\n   - Start from initial state\n   - Expand promising nodes\n   - Return best path found\n\nThis implements LATS tree search for exploring solution space.\n\nAppend tree search lessons to learned_lessons.json.",
    "budget": 2.00,
    "model": "haiku"
  },
  {
    "task": "META: Add adaptive task complexity detection\n\nEnhance grind_spawner.py task classification:\n\n1. Read current classify_task_complexity() method\n2. Improve complexity detection with more signals:\n   - Word count of task description\n   - Presence of 'create', 'implement', 'design' (higher complexity)\n   - Presence of 'fix', 'update', 'add' (lower complexity)\n   - Number of files mentioned\n   - References to papers or architectures\n\n3. Add complexity_score: float 0.0-1.0 (not just simple/complex)\n4. Log complexity score in grind session results\n5. Use score to adjust:\n   - Budget allocation\n   - Model selection (high complexity -> prefer opus)\n   - Role chain length\n\nAppend adaptive complexity lessons to learned_lessons.json.",
    "budget": 2.00,
    "model": "haiku"
  },
  {
    "task": "KNOWLEDGE: Create basic knowledge graph structure\n\nCreate knowledge_graph.py based on HippoRAG patterns:\n\n1. Create knowledge_graph.py with:\n   - KnowledgeNode class: id, label, type, properties\n   - KnowledgeEdge class: source, target, relation\n   - KnowledgeGraph class with add_node(), add_edge(), query()\n\n2. Node types: CONCEPT, SKILL, LESSON, FILE, FUNCTION\n3. Edge types: RELATES_TO, IMPLEMENTS, USES, DEPENDS_ON\n\n4. Add populate_from_codebase():\n   - Scan .py files for function/class definitions\n   - Create FILE and FUNCTION nodes\n   - Link with CONTAINS edges\n\n5. Add query_related(node_id, depth=2) -> subgraph\n\nThis enables concept mapping as foundation for Wave 10.\n\nAppend knowledge graph lessons to learned_lessons.json.",
    "budget": 2.00,
    "model": "haiku"
  }
]
